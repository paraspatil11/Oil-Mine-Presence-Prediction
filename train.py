# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T-omd1TAKOj4wuRXShWgx0wHqhYEiPwk
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from scipy.spatial import KDTree
from torch_geometric.utils import softmax
from torch.optim.lr_scheduler import ReduceLROnPlateau
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_auc_score, average_precision_score,
    matthews_corrcoef, cohen_kappa_score
)
import folium
from folium.plugins import MarkerCluster
import joblib # For saving/loading preprocessing objects
import copy


class LSGATLayer(nn.Module):
    def __init__(self, in_features, out_features, dropout=0.5, alpha=0.2, concat=True, layer_id=1):
        super(LSGATLayer, self).__init__()
        self.concat = concat
        self.dropout = dropout
        self.layer_id = layer_id

        self.W = nn.Parameter(torch.empty(in_features, out_features))
        self.a = nn.Parameter(torch.empty(2 * out_features, 1))

        nn.init.xavier_uniform_(self.W)
        nn.init.xavier_uniform_(self.a)

        self.skip_proj = (
            nn.Linear(in_features, out_features, bias=False)
            if in_features != out_features else nn.Identity()
        )

        self.bn = nn.BatchNorm1d(out_features)
        self.leakyrelu = nn.LeakyReLU(alpha)
        self.gamma = nn.Parameter(torch.tensor(1.0 / layer_id), requires_grad=True)

        self.feature_att = nn.Sequential(
            nn.Linear(out_features, out_features // 2),
            nn.ReLU(),
            nn.Linear(out_features // 2, out_features),
            nn.Sigmoid()
        )

    def forward(self, h, edge_index):
        Wh = torch.mm(h, self.W)
        Wh_i = Wh[edge_index[0]]
        Wh_j = Wh[edge_index[1]]

        edge_input = torch.cat([Wh_i, Wh_j], dim=1)
        e = self.leakyrelu(torch.matmul(edge_input, self.a).squeeze(-1))
        e = F.dropout(e, self.dropout, training=self.training)
        alpha = softmax(e, edge_index[0])

        h_prime = torch.zeros_like(Wh)
        h_prime = h_prime.index_add(0, edge_index[0], alpha.unsqueeze(1) * Wh_j)

        attention_weights = self.feature_att(h_prime)
        h_prime = attention_weights * h_prime

        h_prime = self.bn(h_prime)
        skip = self.skip_proj(h)
        return F.elu(skip + self.gamma * h_prime) if self.concat else (skip + self.gamma * h_prime)

class LSGATNet(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, dropout=0.5, alpha=0.2, heads=8, use_global=True):
        super(LSGATNet, self).__init__()
        self.use_global = use_global
        self.dropout = dropout

        self.attentions = nn.ModuleList([
            LSGATLayer(in_features, hidden_features, dropout, alpha, concat=True, layer_id=1)
            for _ in range(heads)
        ])

        self.middle1 = LSGATLayer(hidden_features * heads, hidden_features, dropout, alpha, concat=True, layer_id=2)
        self.middle2 = LSGATLayer(hidden_features, hidden_features, dropout, alpha, concat=True, layer_id=3)
        self.out_att = LSGATLayer(hidden_features, hidden_features, dropout, alpha, concat=False, layer_id=4)

        if use_global:
            self.global_proj = nn.Sequential(
                nn.Linear(hidden_features * heads, hidden_features),
                nn.Tanh(),
                nn.Dropout(0.3)
            )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_features * (2 if use_global else 1), hidden_features),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_features, out_features)
        )

    def forward(self, x, edge_index):
        x = F.dropout(x, self.dropout, training=self.training)
        x = torch.cat([att(x, edge_index) for att in self.attentions], dim=1)

        global_context = None
        if self.use_global:
            global_context = self.global_proj(x)

        x = self.middle1(x, edge_index)
        x = self.middle2(x, edge_index)
        x = self.out_att(x, edge_index)

        if global_context is not None:
            x = torch.cat([x, global_context], dim=1)

        return F.log_softmax(self.classifier(x), dim=1)

if __name__ == "__main__":
    print("--- Starting LSGAT Training Pipeline ---")

    # --- Data Loading and Preprocessing ---
    print("\n--- Loading and Preparing Data ---")
    df = pd.read_csv("preprocessed_mining_data.csv")

    conditions = (
        (df['P Wave Velocity (km/s)'] < 0.7) &
        (df['S Wave Velocity (km/s)'] < 0.7) &
        (df['Carbon Emission (ppm)'] > 0.3)
    )
    df['oil_mine_presence'] = np.where(conditions, 1, 0)

    categorical_cols = ['hrock_type', 'arock_type', 'structure', 'orebody_fm']
    label_encoders = {col: LabelEncoder() for col in categorical_cols}
    for col in categorical_cols:
        df[col] = label_encoders[col].fit_transform(df[col])

    features = ['latitude', 'longitude', 'P Wave Velocity (km/s)', 'S Wave Velocity (km/s)',
                'Humidity (%)', 'Carbon Emission (ppm)', 'hrock_type', 'arock_type',
                'structure', 'orebody_fm']
    df_features = df[features]
    df_target = df['oil_mine_presence']

    scaler = StandardScaler()
    df_features_scaled = pd.DataFrame(scaler.fit_transform(df_features), columns=features)
    nodes_np = df_features_scaled.values
    y = torch.tensor(df_target.values, dtype=torch.long)

    print(f"Oil mine presence distribution:\n{df['oil_mine_presence'].value_counts()}")

    # --- Graph Construction (KDTree-based KNN) ---
    print("\n--- Constructing Graph ---")
    tree = KDTree(nodes_np)
    k = 10
    distances, indices = tree.query(nodes_np, k=k)

    edges = []
    for i, neighbors in enumerate(indices):
        for j in neighbors:
            if i != j:
                edges.append((i, j))

    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    x = torch.tensor(nodes_np, dtype=torch.float)

    num_nodes = x.shape[0]
    num_edges = edge_index.shape[1]
    print(f"Total nodes: {num_nodes}")
    print(f"Total edges: {num_edges}")
    density = num_edges / (num_nodes * (num_nodes - 1))
    print(f"Edge density: {density:.6f}")

    # --- Prepare Train/Test Split ---
    print("\n--- Preparing Train/Test Split ---")
    data = Data(x=x, edge_index=edge_index, y=y)
    num_nodes = data.num_nodes
    train_mask, test_mask = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42, stratify=y)

    train_mask = torch.tensor(train_mask, dtype=torch.long)
    test_mask = torch.tensor(test_mask, dtype=torch.long)

    # --- Training Setup ---
    print("\n--- Setting up Training ---")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    model = LSGATNet(
        in_features=x.shape[1], hidden_features=32, out_features=2,
        heads=4, dropout=0.5, use_global=True
    ).to(device)

    data = data.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)
    loss_fn = nn.NLLLoss()

    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=10, factor=0.5, verbose=True)
    best_acc = 0
    patience = 20
    epochs_without_improvement = 0
    num_epochs = 201

    # --- Training Loop ---
    print("\n--- Starting Training Loop ---")
    for epoch in range(1, num_epochs):
        model.train()
        dropedge_rate = 0.2
        mask = torch.rand(edge_index.shape[1]) > dropedge_rate
        edge_index_drop = edge_index[:, mask].to(device)

        optimizer.zero_grad()
        out = model(data.x, edge_index_drop)
        loss = loss_fn(out[train_mask], data.y[train_mask])
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            out = model(data.x, data.edge_index)
            log_probs = out[test_mask]
            prob = log_probs.exp()
            _, pred = prob.max(dim=1)
            correct = pred.eq(data.y[test_mask]).sum().item()
            acc = correct / test_mask.size(0)

        scheduler.step(acc)

        if acc > best_acc:
            best_acc = acc
            epochs_without_improvement = 0
            torch.save(model.state_dict(), "best_lsgat_model.pth") # Save best model
        else:
            epochs_without_improvement += 1

        if epoch % 10 == 0 or epoch == 1:
            print(f"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f} | Best: {best_acc:.4f}")

        if epochs_without_improvement >= patience:
            print(f"⏸️ Early stopping at epoch {epoch}. Best accuracy: {best_acc:.4f}")
            break

    print("\n--- Training Complete ---")
    # Load the best model weights for final evaluation and prediction
    model.load_state_dict(torch.load("best_lsgat_model.pth"))
    model.eval()

    # --- Comprehensive Evaluation Metrics ---
    print("\n--- Comprehensive Evaluation Metrics ---")
    with torch.no_grad():
        out = model(data.x, data.edge_index)
        pred_prob = out[test_mask].exp()
        pred_label = pred_prob.argmax(dim=1).cpu().numpy()
        true_label = data.y[test_mask].cpu().numpy()

    tn, fp, fn, tp = confusion_matrix(true_label, pred_label).ravel()

    acc = accuracy_score(true_label, pred_label)
    precision = precision_score(true_label, pred_label)
    recall = recall_score(true_label, pred_label)
    f1 = f1_score(true_label, pred_label)
    roc_auc = roc_auc_score(true_label, pred_prob[:,1].cpu().numpy())
    pr_auc = average_precision_score(true_label, pred_prob[:,1].cpu().numpy())
    mcc = matthews_corrcoef(true_label, pred_label)
    kappa = cohen_kappa_score(true_label, pred_label)
    g_mean = np.sqrt(recall * (tn / (tn + fp)))
    balanced_acc = (recall + (tn / (tn + fp))) / 2
    specificity = tn / (tn + fp)

    print(f"Accuracy:             {acc:.4f}")
    print(f"Precision:            {precision:.4f}")
    print(f"Recall (Sensitivity): {recall:.4f}")
    print(f"Specificity:          {specificity:.4f}")
    print(f"Balanced Accuracy:    {balanced_acc:.4f}")
    print(f"F1 Score:             {f1:.4f}")
    print(f"ROC-AUC:              {roc_auc:.4f}")
    print(f"PR-AUC:               {pr_auc:.4f}")
    print(f"Matthews Corrcoef:    {mcc:.4f}")
    print(f"Cohen's Kappa:        {kappa:.4f}")
    print(f"G-Mean:               {g_mean:.4f}")

    # --- Saving Preprocessing Objects and Final Model State ---
    print("\n--- Saving Preprocessing Objects and Model ---")
    joblib.dump(scaler, 'scaler.pkl')
    joblib.dump(label_encoders, 'label_encoders.pkl')
    joblib.dump(features, 'feature_names.pkl') # Save feature names for consistency
    # best_lsgat_model.pth is already saved during training loop
    print("Scaler, Label Encoders, Feature Names, and Best Model saved.")

    # --- Hyperparameter Sensitivity Analysis ---
    print("\n--- Starting Hyperparameter Sensitivity Analysis ---")
    default_params = {
        'hidden_features': 32,
        'dropout': 0.5,
        'heads': 4,
        'dropedge_rate': 0.2,
        'learning_rate': 0.005
    }

    explore_params = {
        'hidden_features': [8, 16, 32],
        'dropout': [0.3, 0.4, 0.5],
        'heads': [2, 4, 8],
        'dropedge_rate': [0.1, 0.2, 0.3],
        'learning_rate': [0.001, 0.005, 0.01]
    }

    results = []

    def train_and_evaluate_for_sensitivity(params):
        hidden_features = params['hidden_features']
        dropout = params['dropout']
        heads = params['heads']
        dropedge_rate = params['dropedge_rate']
        lr = params['learning_rate']

        model_sensitivity = LSGATNet(
            in_features=x.shape[1],
            hidden_features=hidden_features,
            out_features=2,
            heads=heads,
            dropout=dropout,
            use_global=True
        ).to(device)

        optimizer_sensitivity = torch.optim.Adam(model_sensitivity.parameters(), lr=lr, weight_decay=5e-4)
        loss_fn_sensitivity = nn.NLLLoss()

        for epoch in range(1, 51):
            model_sensitivity.train()
            mask_sensitivity = torch.rand(edge_index.shape[1]) > dropedge_rate
            edge_index_drop_sensitivity = edge_index[:, mask_sensitivity].to(device)

            optimizer_sensitivity.zero_grad()
            out_sensitivity = model_sensitivity(data.x, edge_index_drop_sensitivity)
            loss_sensitivity = loss_fn_sensitivity(out_sensitivity[train_mask], data.y[train_mask])
            loss_sensitivity.backward()
            optimizer_sensitivity.step()

        model_sensitivity.eval()
        with torch.no_grad():
            out_sensitivity = model_sensitivity(data.x, data.edge_index)
            pred_sensitivity = out_sensitivity[test_mask].exp().argmax(dim=1)
            acc_sensitivity = (pred_sensitivity == data.y[test_mask]).float().mean().item()

        return acc_sensitivity

    for param_name, values in explore_params.items():
        print(f"\nExploring parameter: {param_name}")
        for val in values:
            test_params = copy.deepcopy(default_params)
            test_params[param_name] = val

            acc = train_and_evaluate_for_sensitivity(test_params)
            results.append({
                'Parameter': param_name,
                'Value': val,
                'Test Accuracy': acc
            })
            print(f"{param_name} = {val} --> Test Accuracy: {acc:.4f}")

    df_results = pd.DataFrame(results)
    print("\n=== Hyperparameter Sensitivity Results ===")
    print(df_results)

    df_results.to_csv("hyperparameter_sensitivity_results.csv", index=False)

    print("\n--- Generating Hyperparameter Sensitivity Plots ---")
    for param in explore_params.keys():
        plt.figure(figsize=(8, 5))
        subset = df_results[df_results['Parameter'] == param]
        sns.lineplot(x='Value', y='Test Accuracy', data=subset, marker='o')
        plt.title(f"Test Accuracy vs {param}")
        plt.xlabel(param)
        plt.ylabel("Test Accuracy")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(f"sensitivity_plot_{param}.png")
        plt.close() # Close plot to free memory
    print("Sensitivity plots saved.")

    # --- Folium Map Visualization of Predictions ---
    print("\n--- Generating Folium Maps ---")

    model.eval()
    with torch.no_grad():
        final_pred = model(data.x, data.edge_index).argmax(dim=1)

    true_labels_all = data.y.cpu().numpy()
    pred_labels_all = final_pred.cpu().numpy()

    test_indices = test_mask.cpu().numpy()
    df_display = df.iloc[test_indices].copy()

    df_display["true_label"] = true_labels_all[test_indices]
    df_display["pred_label"] = pred_labels_all[test_indices]

    map_center = [df["latitude"].mean(), df["longitude"].mean()]
    m1 = folium.Map(location=map_center, zoom_start=6)
    marker_cluster1 = MarkerCluster().add_to(m1)

    for _, row in df_display[df_display["true_label"] == 1].iterrows():
        folium.CircleMarker(
            location=[row["latitude"], row["longitude"]],
            radius=4,
            color="green",
            fill=True,
            fill_color="green",
            popup="Actual Oil Mine"
        ).add_to(marker_cluster1)

    for _, row in df_display[(df_display["true_label"] == 0) & (df_display["pred_label"] == 1)].iterrows():
        folium.CircleMarker(
            location=[row["latitude"], row["longitude"]],
            radius=6,
            color="red",
            fill=True,
            fill_color="red",
            popup="Predicted Oil Mine (False Positive)"
        ).add_to(marker_cluster1)

    m1.save("actual_vs_predicted_oil_mines_map.html")
    print("Map saved to 'actual_vs_predicted_oil_mines_map.html'")

    m2 = folium.Map(location=[df_display["latitude"].mean(), df_display["longitude"].mean()], zoom_start=3)

    for _, row in df_display.iterrows():
        color = ""
        popup_status = ""
        if row["true_label"] == row["pred_label"] == 1:
            color = "green"
            popup_status = "Correct (Oil Mine)"
        elif row["true_label"] == row["pred_label"] == 0:
            color = "blue"
            popup_status = "Correct (No Mine)"
        else:
            color = "red"
            popup_status = "Incorrect Prediction"

        popup_text = (
            f'True: {row["true_label"]}, Pred: {row["pred_label"]}<br>'
            f'Status: {popup_status}<br>'
            f'Latitude: {row["latitude"]:.4f}, Longitude: {row["longitude"]:.4f}'
        )

        folium.CircleMarker(
            location=[row["latitude"], row["longitude"]],
            radius=4,
            color=color,
            fill=True,
            fill_opacity=0.7,
            fill_color=color,
            popup=popup_text,
        ).add_to(m2)

    m2.save("all_test_predictions_map.html")
    print("Map saved to 'all_test_predictions_map.html'")

    print("\n--- LSGAT Training Pipeline Finished ---")