# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UJxGFVQORz8N6T9picQG_Zeduxh1SU2u
"""

import streamlit as st
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from scipy.spatial import KDTree
import joblib
import folium
from folium.plugins import MarkerCluster
from torch_geometric.utils import softmax # Used in LSGATLayer


class LSGATLayer(nn.Module):
    def __init__(self, in_features, out_features, dropout=0.5, alpha=0.2, concat=True, layer_id=1):
        super(LSGATLayer, self).__init__()
        self.concat = concat
        self.dropout = dropout
        self.layer_id = layer_id

        self.W = nn.Parameter(torch.empty(in_features, out_features))
        self.a = nn.Parameter(torch.empty(2 * out_features, 1))

        nn.init.xavier_uniform_(self.W)
        nn.init.xavier_uniform_(self.a)

        self.skip_proj = (
            nn.Linear(in_features, out_features, bias=False)
            if in_features != out_features else nn.Identity()
        )

        self.bn = nn.BatchNorm1d(out_features)
        self.leakyrelu = nn.LeakyReLU(alpha)
        self.gamma = nn.Parameter(torch.tensor(1.0 / layer_id), requires_grad=True)

        self.feature_att = nn.Sequential(
            nn.Linear(out_features, out_features // 2),
            nn.ReLU(),
            nn.Linear(out_features // 2, out_features),
            nn.Sigmoid()
        )

    def forward(self, h, edge_index):
        Wh = torch.mm(h, self.W)
        Wh_i = Wh[edge_index[0]]
        Wh_j = Wh[edge_index[1]]

        edge_input = torch.cat([Wh_i, Wh_j], dim=1)
        e = self.leakyrelu(torch.matmul(edge_input, self.a).squeeze(-1))
        e = F.dropout(e, self.dropout, training=self.training)
        alpha = softmax(e, edge_index[0])

        h_prime = torch.zeros_like(Wh)
        h_prime = h_prime.index_add(0, edge_index[0], alpha.unsqueeze(1) * Wh_j)

        attention_weights = self.feature_att(h_prime)
        h_prime = attention_weights * h_prime

        h_prime = self.bn(h_prime)
        skip = self.skip_proj(h)
        return F.elu(skip + self.gamma * h_prime) if self.concat else (skip + self.gamma * h_prime)

class LSGATNet(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, dropout=0.5, alpha=0.2, heads=8, use_global=True):
        super(LSGATNet, self).__init__()
        self.use_global = use_global
        self.dropout = dropout

        self.attentions = nn.ModuleList([
            LSGATLayer(in_features, hidden_features, dropout, alpha, concat=True, layer_id=1)
            for _ in range(heads)
        ])

        self.middle1 = LSGATLayer(hidden_features * heads, hidden_features, dropout, alpha, concat=True, layer_id=2)
        self.middle2 = LSGATLayer(hidden_features, hidden_features, dropout, alpha, concat=True, layer_id=3)
        self.out_att = LSGATLayer(hidden_features, hidden_features, dropout, alpha, concat=False, layer_id=4)

        if use_global:
            self.global_proj = nn.Sequential(
                nn.Linear(hidden_features * heads, hidden_features),
                nn.Tanh(),
                nn.Dropout(0.3)
            )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_features * (2 if use_global else 1), hidden_features),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_features, out_features)
        )

    def forward(self, x, edge_index):
        x = F.dropout(x, self.dropout, training=self.training)
        x = torch.cat([att(x, edge_index) for att in self.attentions], dim=1)

        global_context = None
        if self.use_global:
            global_context = self.global_proj(x)

        x = self.middle1(x, edge_index)
        x = self.middle2(x, edge_index)
        x = self.out_att(x, edge_index)

        if global_context is not None:
            x = torch.cat([x, global_context], dim=1)

        return F.log_softmax(self.classifier(x), dim=1)

@st.cache_resource
def load_resources():
    try:
        # Load scaler and label encoders
        scaler = joblib.load('scaler.pkl')
        label_encoders = joblib.load('label_encoders.pkl')
        feature_names = joblib.load('feature_names.pkl')

        # Load original training data for KDTree
        df_full = pd.read_csv("preprocessed_mining_data.csv")
        # Ensure 'oil_mine_presence' is defined as in training for consistency
        conditions = (
            (df_full['P Wave Velocity (km/s)'] < 0.7) &
            (df_full['S Wave Velocity (km/s)'] < 0.7) &
            (df['Carbon Emission (ppm)'] > 0.3)
        )
        df_full['oil_mine_presence'] = np.where(conditions, 1, 0)

        # Re-encode categorical features in df_full using the loaded encoders
        for col, encoder in label_encoders.items():
            df_full[col] = encoder.transform(df_full[col])

        # Scale features in df_full using the loaded scaler
        nodes_np_full = scaler.transform(df_full[feature_names])
        full_tree = KDTree(nodes_np_full) # KDTree of the full training data
        full_nodes_tensor = torch.tensor(nodes_np_full, dtype=torch.float)

        # Initialize model (parameters must match training parameters)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = LSGATNet(
            in_features=len(feature_names), hidden_features=32, out_features=2,
            heads=4, dropout=0.5, use_global=True
        ).to(device)
        model.load_state_dict(torch.load('best_lsgat_model.pth', map_location=device))
        model.eval() # Set model to evaluation mode

        return scaler, label_encoders, feature_names, model, full_tree, full_nodes_tensor, device

    except FileNotFoundError as e:
        st.error(f"Required file not found: {e}. Please ensure 'scaler.pkl', 'label_encoders.pkl', 'feature_names.pkl', 'best_lsgat_model.pth', and 'preprocessed_mining_data.csv' are in the same directory. Run 'python train.py' first.")
        st.stop()
    except Exception as e:
        st.error(f"An error occurred during resource loading: {e}. Please check your files and dependencies.")
        st.stop()

scaler, label_encoders, feature_names, model, full_tree, full_nodes_tensor, device = load_resources()

st.set_page_config(layout="wide")
st.title("Oil Mine Presence Prediction using LSGAT")
st.markdown("""
    Enter the geological and environmental parameters below to predict the likelihood of oil mine presence.
    The model uses a pre-trained Graph Attention Network (LSGAT).
""")

st.header("Input Parameters")

col1, col2 = st.columns(2)

input_data = {}

with col1:
    st.subheader("Geographical & Velocity Data")
    input_data['latitude'] = st.number_input("Latitude", value=0.0, format="%.6f", help="Geographical latitude")
    input_data['longitude'] = st.number_input("Longitude", value=0.0, format="%.6f", help="Geographical longitude")
    input_data['P Wave Velocity (km/s)'] = st.number_input("P Wave Velocity (km/s)", value=0.5, min_value=0.0, max_value=10.0, format="%.3f", help="Compressional wave velocity in km/s")
    input_data['S Wave Velocity (km/s)'] = st.number_input("S Wave Velocity (km/s)", value=0.5, min_value=0.0, max_value=10.0, format="%.3f", help="Shear wave velocity in km/s")
    input_data['Humidity (%)'] = st.number_input("Humidity (%)", value=50.0, min_value=0.0, max_value=100.0, format="%.1f", help="Relative humidity in percentage")

with col2:
    st.subheader("Environmental & Rock Data")
    input_data['Carbon Emission (ppm)'] = st.number_input("Carbon Emission (ppm)", value=0.2, min_value=0.0, max_value=1.0, format="%.3f", help="Carbon emission in parts per million")

    hrock_type_options = list(label_encoders['hrock_type'].classes_)
    arock_type_options = list(label_encoders['arock_type'].classes_)
    structure_options = list(label_encoders['structure'].classes_)
    orebody_fm_options = list(label_encoders['orebody_fm'].classes_)

    input_data['hrock_type'] = st.selectbox("Host Rock Type", hrock_type_options, help="Type of rock surrounding the orebody")
    input_data['arock_type'] = st.selectbox("Associated Rock Type", arock_type_options, help="Associated rock type found with the orebody")
    input_data['structure'] = st.selectbox("Geological Structure", structure_options, help="Dominant geological structure")
    input_data['orebody_fm'] = st.selectbox("Orebody Formation", orebody_fm_options, help="Formation type of the orebody")


if st.button("Predict Oil Mine Presence"):
    # --- Preprocess User Input ---
    for col in ['hrock_type', 'arock_type', 'structure', 'orebody_fm']:
        input_data[col] = label_encoders[col].transform([input_data[col]])[0]

    input_df = pd.DataFrame([input_data])[feature_names]
    input_scaled = scaler.transform(input_df)
    new_node_tensor = torch.tensor(input_scaled, dtype=torch.float).to(device)

    # --- Dynamic Graph Construction for Single Prediction ---
    k_neighbors = 10 # Must match the k used during training graph construction
    distances, indices = full_tree.query(input_scaled, k=k_neighbors)

    neighbor_original_indices = indices[0]
    neighbor_original_indices = np.array(list(set(neighbor_original_indices)))

    x_combined_inference = torch.cat([new_node_tensor, full_nodes_tensor[neighbor_original_indices]], dim=0)

    edge_index_pred_list = []
    for i in range(len(neighbor_original_indices)):
        edge_index_pred_list.append([0, i + 1]) # New node to neighbor
        edge_index_pred_list.append([i + 1, 0]) # Neighbor to new node (bidirectional)

    if not edge_index_pred_list:
        st.warning("Could not find sufficient neighbors for graph construction. Prediction might be less accurate.")
        edge_index_pred = torch.empty((2,0), dtype=torch.long).to(device)
    else:
        edge_index_pred = torch.tensor(edge_index_pred_list, dtype=torch.long).t().contiguous().to(device)


    # --- Make Prediction ---
    model.eval()
    with torch.no_grad():
        out = model(x_combined_inference, edge_index_pred)
        log_probs = out[0] # Prediction for the new node is at index 0
        probabilities = torch.exp(log_probs)
        predicted_class = probabilities.argmax().item()
        confidence = probabilities[predicted_class].item()

    # --- Display Results ---
    st.subheader("Prediction Result")
    if predicted_class == 1:
        st.success(f"**Predicted Oil Mine Presence: YES** (Confidence: {confidence:.2f})")
        st.balloons()
    else:
        st.info(f"**Predicted Oil Mine Presence: NO** (Confidence: {confidence:.2f})")

    st.write(f"Raw Probabilities: Class 0: {probabilities[0]:.2f}, Class 1: {probabilities[1]:.2f}")
    st.write("---")

    # --- Visualize on Map ---
    st.subheader("Predicted Location on Map")

    map_center = [input_data['latitude'], input_data['longitude']]
    m = folium.Map(location=map_center, zoom_start=10)

    color = "green" if predicted_class == 1 else "blue"
    popup_text = f"Predicted: {'Oil Mine' if predicted_class == 1 else 'No Mine'}<br>Confidence: {confidence:.2f}"

    folium.CircleMarker(
        location=[input_data['latitude'], input_data['longitude']],
        radius=8,
        color=color,
        fill=True,
        fill_color=color,
        fill_opacity=0.8,
        popup=popup_text
    ).add_to(m)

    df_full_display_original = pd.read_csv("preprocessed_mining_data.csv")

    if len(neighbor_original_indices) > 0:
        neighbor_df = df_full_display_original.iloc[neighbor_original_indices]
        for idx, row in neighbor_df.iterrows():
            folium.CircleMarker(
                location=[row['latitude'], row['longitude']],
                radius=3,
                color='gray',
                fill=True,
                fill_color='gray',
                fill_opacity=0.5,
                popup=f"Neighbor from Training Data"
            ).add_to(m)

    st_data = folium.Choropleth(geo_data=m.get_json())._repr_html_()
    st.components.v1.html(st_data, width=700, height=500)